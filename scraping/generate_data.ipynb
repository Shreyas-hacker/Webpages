{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4 as bs4\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website_url</th>\n",
       "      <th>cleaned_website_text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>https://adultcompass.com/avs/0/1/acw/index.html</td>\n",
       "      <td>crazy d sex worldthe best frames of the frank ...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>http://www.tentacle-hentai.com/</td>\n",
       "      <td>tentacle hentai sex tentacle monsters fucking ...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>https://www.hotsexsites.org/blog/</td>\n",
       "      <td>blog hot sex sitesblog how much are stripchat ...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>http://www.allanimatedtgp.com/avs/cartoonposer</td>\n",
       "      <td>all animated tgpthis page doesnt existvideos c...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>http://www.allanimatedtgp.com/tags</td>\n",
       "      <td>all animated tgp video categoriesall animated ...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>http://www.allanimatedtgp.com/videos?t=9</td>\n",
       "      <td>amateur videos all animated tgpview over amate...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>https://www.hentagon.com/</td>\n",
       "      <td>free hentai porn videos xxx anime sexlove hent...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>https://www.banzaihentai.net/</td>\n",
       "      <td>banzaihentai discover new pleasuresdiscover ne...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>https://www.fandomination.net/</td>\n",
       "      <td>fan domination sex cams shows bdsm girlfan do...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>http://www.abcb.com/parents/legend.htm</td>\n",
       "      <td>the animé café a parents guide to animehelp ur...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         website_url  \\\n",
       "923  https://adultcompass.com/avs/0/1/acw/index.html   \n",
       "924                  http://www.tentacle-hentai.com/   \n",
       "925                https://www.hotsexsites.org/blog/   \n",
       "926   http://www.allanimatedtgp.com/avs/cartoonposer   \n",
       "927               http://www.allanimatedtgp.com/tags   \n",
       "928         http://www.allanimatedtgp.com/videos?t=9   \n",
       "929                        https://www.hentagon.com/   \n",
       "930                    https://www.banzaihentai.net/   \n",
       "931                   https://www.fandomination.net/   \n",
       "932           http://www.abcb.com/parents/legend.htm   \n",
       "\n",
       "                                  cleaned_website_text Category  \n",
       "923  crazy d sex worldthe best frames of the frank ...    Adult  \n",
       "924  tentacle hentai sex tentacle monsters fucking ...    Adult  \n",
       "925  blog hot sex sitesblog how much are stripchat ...    Adult  \n",
       "926  all animated tgpthis page doesnt existvideos c...    Adult  \n",
       "927  all animated tgp video categoriesall animated ...    Adult  \n",
       "928  amateur videos all animated tgpview over amate...    Adult  \n",
       "929  free hentai porn videos xxx anime sexlove hent...    Adult  \n",
       "930  banzaihentai discover new pleasuresdiscover ne...    Adult  \n",
       "931   fan domination sex cams shows bdsm girlfan do...    Adult  \n",
       "932  the animé café a parents guide to animehelp ur...    Adult  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_row', 10)\n",
    "df = pd.read_csv('../website_classification.csv')\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website_url</th>\n",
       "      <th>cleaned_website_text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.kasrl.org/jaffe.html</td>\n",
       "      <td>japanese female facial expression jaffe datase...</td>\n",
       "      <td>Computers and Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.gmdhshell.com/</td>\n",
       "      <td>best predictive analytic software free academi...</td>\n",
       "      <td>Computers and Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://en.wikipedia.org/wiki/Neural_network</td>\n",
       "      <td>neural network wikipedia neural network conten...</td>\n",
       "      <td>Computers and Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://cadingandcoding.blogspot.com/</td>\n",
       "      <td>cading coding cading coding tuesday march auto...</td>\n",
       "      <td>Computers and Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.panzercad.com/</td>\n",
       "      <td>welcome bluehost rate web host provider free c...</td>\n",
       "      <td>Computers and Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>https://kryptomixer.io/</td>\n",
       "      <td>cryptomixer blender bitcoin mixer service cryp...</td>\n",
       "      <td>Cryptocurrency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>https://www.walletexplorer.com/</td>\n",
       "      <td>walletexplorercom smart bitcoin block explorer...</td>\n",
       "      <td>Cryptocurrency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>https://ultramixer.net/</td>\n",
       "      <td>ultramixer bitcoin mixer bitcoin tumbler best...</td>\n",
       "      <td>Cryptocurrency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>https://nakedsecurity.sophos.com/2020/02/17/po...</td>\n",
       "      <td>police bust alleged operator of bitcoin mixing...</td>\n",
       "      <td>Cryptocurrency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>https://anonymixer.net/</td>\n",
       "      <td>anonymixer best bitcoin mixerbitcoin mixing bi...</td>\n",
       "      <td>Cryptocurrency</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>317 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           website_url  \\\n",
       "0                      http://www.kasrl.org/jaffe.html   \n",
       "1                           https://www.gmdhshell.com/   \n",
       "2          http://en.wikipedia.org/wiki/Neural_network   \n",
       "3                 http://cadingandcoding.blogspot.com/   \n",
       "4                            http://www.panzercad.com/   \n",
       "..                                                 ...   \n",
       "312                            https://kryptomixer.io/   \n",
       "313                    https://www.walletexplorer.com/   \n",
       "314                            https://ultramixer.net/   \n",
       "315  https://nakedsecurity.sophos.com/2020/02/17/po...   \n",
       "316                            https://anonymixer.net/   \n",
       "\n",
       "                                  cleaned_website_text  \\\n",
       "0    japanese female facial expression jaffe datase...   \n",
       "1    best predictive analytic software free academi...   \n",
       "2    neural network wikipedia neural network conten...   \n",
       "3    cading coding cading coding tuesday march auto...   \n",
       "4    welcome bluehost rate web host provider free c...   \n",
       "..                                                 ...   \n",
       "312  cryptomixer blender bitcoin mixer service cryp...   \n",
       "313  walletexplorercom smart bitcoin block explorer...   \n",
       "314   ultramixer bitcoin mixer bitcoin tumbler best...   \n",
       "315  police bust alleged operator of bitcoin mixing...   \n",
       "316  anonymixer best bitcoin mixerbitcoin mixing bi...   \n",
       "\n",
       "                     Category  \n",
       "0    Computers and Technology  \n",
       "1    Computers and Technology  \n",
       "2    Computers and Technology  \n",
       "3    Computers and Technology  \n",
       "4    Computers and Technology  \n",
       "..                        ...  \n",
       "312            Cryptocurrency  \n",
       "313            Cryptocurrency  \n",
       "314            Cryptocurrency  \n",
       "315            Cryptocurrency  \n",
       "316            Cryptocurrency  \n",
       "\n",
       "[317 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computers = pd.read_csv('../Hierarchal model/Computer/computer_subcategory_classification.csv')\n",
    "computers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1562978 entries, 0 to 1562977\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   0       1562978 non-null  int64 \n",
      " 1   1       1562975 non-null  object\n",
      " 2   2       1562978 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 35.8+ MB\n"
     ]
    }
   ],
   "source": [
    "urls = pd.read_csv(\"../URL Classification.csv\",header=None)\n",
    "urls.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arts         253840\n",
       "Society      243943\n",
       "Business     240177\n",
       "Computers    117962\n",
       "Science      110286\n",
       "              ...  \n",
       "Games         56477\n",
       "Kids          46182\n",
       "Adult         35325\n",
       "Home          28269\n",
       "News           8989\n",
       "Name: 2, Length: 15, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[2].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import bs4 as bs4\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "\n",
    "\n",
    "class ScrapTool:   \n",
    "    def visit_url(self,website_url):\n",
    "        '''\n",
    "        Visit URL. Download the Content. Initialize the beautifulsoup object. Call parsing methods. Return Series object.\n",
    "        '''\n",
    "        #Set up Selenium webdriver\n",
    "        PATH = 'C:/Users/shrey/Downloads/chromedriver_win32 (1)/chromedriver.exe'\n",
    "\n",
    "        # Set the Chrome webdriver options\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        # Initialize the Chrome webdriver and pass the Chrome options\n",
    "        service = Service(PATH)\n",
    "        driver = webdriver.Chrome(service = service, options = chrome_options)\n",
    "\n",
    "        #Load website\n",
    "        driver.get(website_url)\n",
    "        \n",
    "        # Check if the cookie consent button is present\n",
    "        cookie_button = None\n",
    "        try:\n",
    "            cookie_button = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"cookie-consent-button\")))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Handle cookies if the button is present\n",
    "        if cookie_button:\n",
    "            cookie_button.click()\n",
    "\n",
    "        # Extract HTML content\n",
    "        html_content = driver.page_source\n",
    "        driver.quit()\n",
    "\n",
    "        soup = BeautifulSoup(html_content,'lxml')\n",
    "        result = {\n",
    "            \"website_url\": website_url,\n",
    "            \"website_name\": self.get_website_name(website_url),\n",
    "            \"website_text\": self.get_html_title_tag(soup)+self.get_html_meta_tags(soup)+self.get_html_heading_tags(soup)+\n",
    "                                    self.get_text_content(soup)\n",
    "        }\n",
    "        \n",
    "        #Convert to Series object and return\n",
    "        return pd.Series(result)\n",
    "    \n",
    "    def get_website_name(self,website_url):\n",
    "        '''\n",
    "        Example: returns \"google\" from \"www.google.com\"\n",
    "        '''\n",
    "        return \"\".join(urlparse(website_url).netloc.split(\".\")[-2])\n",
    "    \n",
    "    def get_html_title_tag(self,soup):\n",
    "        '''Return the text content of <title> tag from a webpage'''\n",
    "        return '. '.join(soup.title.contents)\n",
    "    \n",
    "    def get_html_meta_tags(self,soup):\n",
    "        '''Returns the text content of <meta> tags related to keywords and description from a webpage'''\n",
    "        tags = soup.find_all(lambda tag: (tag.name==\"meta\") & (tag.has_attr('name') & (tag.has_attr('content'))))\n",
    "        content = [str(tag[\"content\"]) for tag in tags if tag[\"name\"] in ['keywords','description']]\n",
    "        return ' '.join(content)\n",
    "    \n",
    "    def get_html_heading_tags(self,soup):\n",
    "        '''returns the text content of heading tags. The assumption is that headings might contain relatively important text.'''\n",
    "        tags = soup.find_all([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "        content = [\" \".join(tag.stripped_strings) for tag in tags]\n",
    "        return ' '.join(content)\n",
    "    \n",
    "    def get_text_content(self,soup):\n",
    "        '''returns the text content of the whole page with some exception to tags. See tags_to_ignore.'''\n",
    "        tags_to_ignore = ['style', 'script', 'head', 'title', 'meta', '[document]',\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\",\"noscript\"]\n",
    "        tags = soup.find_all(text=True)\n",
    "        result = []\n",
    "        for tag in tags:\n",
    "            stripped_tag = tag.strip()\n",
    "            if tag.parent.name not in tags_to_ignore\\\n",
    "                and isinstance(tag, bs4.element.Comment)==False\\\n",
    "                and not stripped_tag.isnumeric()\\\n",
    "                and len(stripped_tag)>0:\n",
    "                result.append(stripped_tag)\n",
    "        return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = text.replace('\\t',' ')\n",
    "    text = text.replace('\\r',' ')\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_generation(website):\n",
    "    try:\n",
    "        scrapTool = ScrapTool()\n",
    "        web = scrapTool.visit_url(website)\n",
    "        text = cleaning_text(web['website_text'])\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print('Error: ',website)\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275    http://groups.yahoo.com/group/furuba_lemon_bas...\n",
       "276    http://www.fandomination.net/?mode=navigation&...\n",
       "277    http://www.angelfire.com/anime/saotomechan/swe...\n",
       "278                http://www.angelfire.com/gundam/fics/\n",
       "279       http://www.geocities.com/karaboo1x2/index.html\n",
       "280               http://www.animetric.com/klm/kite.html\n",
       "281    http://www.geocities.com/sped_reject2002/kite....\n",
       "282    http://www.hentaineko.com/html_peek/peek_anime...\n",
       "283    http://www.hentaineko.com/html_peek/peek_anime...\n",
       "284                http://www.animetric.com/klm/lbg.html\n",
       "285      http://www.geocities.com/tokyo/8197/lbgfaq.html\n",
       "286              http://anime.mikomi.org/series/164.html\n",
       "287                          http://www.urotsukidoji.de/\n",
       "288                  http://www.animejin.org.uk/urot.htm\n",
       "289    http://www.wired.com/wired/archive/2.06/street...\n",
       "290    http://www.thecinemalaser.com/ld_reviews/urots...\n",
       "291               http://www.abcb.com/parents/legend.htm\n",
       "292    http://www.moviesforguys.com/sci-fi/reviews/ov...\n",
       "293                http://www.animetric.com/tuv/uro.html\n",
       "294    http://www.btinternet.com/~huxley_boredom/a_re...\n",
       "295    http://www.hentaineko.com/html_peek/peek_anime...\n",
       "296                  http://www.geocities.com/iquoteron/\n",
       "297                     http://omi_tsukiyono4.tripod.com\n",
       "298                         http://www.ranma-hentai.com/\n",
       "299          http://www.hentai-sailormoon.com/index.html\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "adult_100 = pd.DataFrame(urls[urls[2]==\"Adult\"][1][:300])\n",
    "adult_100[1][275:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp/ipykernel_44972/1580991065.py:84: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  tags = soup.find_all(text=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  http://www.internetdump.com/adult.html\n",
      "'NoneType' object has no attribute 'contents'\n",
      "Error:  \n",
      "Message: invalid argument\n",
      "  (Session info: headless chrome=114.0.5735.134)\n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tGetHandleVerifier [0x00B8A813+48355]\n",
      "\t(No symbol) [0x00B1C4B1]\n",
      "\t(No symbol) [0x00A25220]\n",
      "\t(No symbol) [0x00A188E2]\n",
      "\t(No symbol) [0x00A17138]\n",
      "\t(No symbol) [0x00A17688]\n",
      "\t(No symbol) [0x00A26B52]\n",
      "\t(No symbol) [0x00A7CF01]\n",
      "\t(No symbol) [0x00A6A73C]\n",
      "\t(No symbol) [0x00A7C922]\n",
      "\t(No symbol) [0x00A6A536]\n",
      "\t(No symbol) [0x00A482DC]\n",
      "\t(No symbol) [0x00A493DD]\n",
      "\tGetHandleVerifier [0x00DEAABD+2539405]\n",
      "\tGetHandleVerifier [0x00E2A78F+2800735]\n",
      "\tGetHandleVerifier [0x00E2456C+2775612]\n",
      "\tGetHandleVerifier [0x00C151E0+616112]\n",
      "\t(No symbol) [0x00B25F8C]\n",
      "\t(No symbol) [0x00B22328]\n",
      "\t(No symbol) [0x00B2240B]\n",
      "\t(No symbol) [0x00B14FF7]\n",
      "\tBaseThreadInitThunk [0x76377D59+25]\n",
      "\tRtlInitializeExceptionChain [0x7709B74B+107]\n",
      "\tRtlClearBits [0x7709B6CF+191]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "websites = [\n",
    "    \"https://en.wikipedia.org/wiki/Carding_(fraud)#Carding_on_Telegram\"\n",
    "]\n",
    "\n",
    "for website in websites:\n",
    "    try:\n",
    "        content = content_generation(website)\n",
    "        content_list = [website,content,\"Financial Crime\"]\n",
    "        df.loc[len(df)] = content_list\n",
    "    except Exception as e:\n",
    "        print(website,\": \",e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computers and Technology           303\n",
       "Social Networking and Messaging    114\n",
       "Business/Corporate                 106\n",
       "E-Commerce                         101\n",
       "News                                93\n",
       "Law and Government                  83\n",
       "Narcotics                           74\n",
       "Adult                               43\n",
       "Forums                              16\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"website_classification.csv\",index=False)\n",
    "# computers.to_csv(\"../Hierarchal model/Computer/computer_subcategory_classification.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
