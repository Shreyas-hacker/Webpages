{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4 as bs4\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website_url</th>\n",
       "      <th>cleaned_website_text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>http://plasticwrevezcuvtt6aj2bl3xx5q4x4wm2ptlf...</td>\n",
       "      <td>plastic marketplacechoose plasticcloned cards ...</td>\n",
       "      <td>Financial Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>http://ccpalymn3bi5uctrujnvrp5lf2nmyqf37oykanp...</td>\n",
       "      <td>ccpal ccs cvvs paypals ebay accounts and more ...</td>\n",
       "      <td>Financial Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>http://imperiacfwnxzhkpyce2ifxwdf3zromko5wwrba...</td>\n",
       "      <td>imperial market the financial servicecard card...</td>\n",
       "      <td>Financial Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>http://acteam2mf2ubo5hapf2dgsj4h7kjembfy4c6pml...</td>\n",
       "      <td>card the world verified sellersstore for credi...</td>\n",
       "      <td>Financial Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>http://blackstgivygqcitkvmi6qwta3fpxhaakdjbhfe...</td>\n",
       "      <td>black store worldwide credit cards what we off...</td>\n",
       "      <td>Financial Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>http://ccvendoqij2gteltjdd64usgdparw6kq2c2aqgm...</td>\n",
       "      <td>cc vendor qualityour prices usd cards eur card...</td>\n",
       "      <td>Financial Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>https://ascarding.com/</td>\n",
       "      <td>the carding asylumcarding hacking vendors card...</td>\n",
       "      <td>Financial Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>https://ascarding.com/forums/16/</td>\n",
       "      <td>cardable sites the carding asylumcardable site...</td>\n",
       "      <td>Financial Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>https://ascarding.com/forums/19/</td>\n",
       "      <td>bank carding the carding asylumbank carding go...</td>\n",
       "      <td>Financial Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>https://cardvilla.cc/carding-forum/?s=76185e0a...</td>\n",
       "      <td>carding forumcardingforumcarding carding carde...</td>\n",
       "      <td>Financial Crime</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           website_url  \\\n",
       "953  http://plasticwrevezcuvtt6aj2bl3xx5q4x4wm2ptlf...   \n",
       "954  http://ccpalymn3bi5uctrujnvrp5lf2nmyqf37oykanp...   \n",
       "955  http://imperiacfwnxzhkpyce2ifxwdf3zromko5wwrba...   \n",
       "956  http://acteam2mf2ubo5hapf2dgsj4h7kjembfy4c6pml...   \n",
       "957  http://blackstgivygqcitkvmi6qwta3fpxhaakdjbhfe...   \n",
       "958  http://ccvendoqij2gteltjdd64usgdparw6kq2c2aqgm...   \n",
       "959                             https://ascarding.com/   \n",
       "960                   https://ascarding.com/forums/16/   \n",
       "961                   https://ascarding.com/forums/19/   \n",
       "962  https://cardvilla.cc/carding-forum/?s=76185e0a...   \n",
       "\n",
       "                                  cleaned_website_text         Category  \n",
       "953  plastic marketplacechoose plasticcloned cards ...  Financial Crime  \n",
       "954  ccpal ccs cvvs paypals ebay accounts and more ...  Financial Crime  \n",
       "955  imperial market the financial servicecard card...  Financial Crime  \n",
       "956  card the world verified sellersstore for credi...  Financial Crime  \n",
       "957  black store worldwide credit cards what we off...  Financial Crime  \n",
       "958  cc vendor qualityour prices usd cards eur card...  Financial Crime  \n",
       "959  the carding asylumcarding hacking vendors card...  Financial Crime  \n",
       "960  cardable sites the carding asylumcardable site...  Financial Crime  \n",
       "961  bank carding the carding asylumbank carding go...  Financial Crime  \n",
       "962  carding forumcardingforumcarding carding carde...  Financial Crime  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_row', 10)\n",
    "df = pd.read_csv('../website_classification.csv')\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import bs4 as bs4\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "\n",
    "\n",
    "class ScrapTool:   \n",
    "    def visit_url(self,website_url):\n",
    "        '''\n",
    "        Visit URL. Download the Content. Initialize the beautifulsoup object. Call parsing methods. Return Series object.\n",
    "        '''\n",
    "        #Set up Selenium webdriver\n",
    "        PATH = 'C:/Users/User01/Downloads/chromedriver_win32/chromedriver.exe'\n",
    "\n",
    "        # Set the Chrome webdriver options\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        # Initialize the Chrome webdriver and pass the Chrome options\n",
    "        service = Service(PATH)\n",
    "        driver = webdriver.Chrome(service = service, options = chrome_options)\n",
    "\n",
    "        #Load website\n",
    "        driver.get(website_url)\n",
    "        \n",
    "        # Check if the cookie consent button is present\n",
    "        cookie_button = None\n",
    "        try:\n",
    "            cookie_button = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"cookie-consent-button\")))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Handle cookies if the button is present\n",
    "        if cookie_button:\n",
    "            cookie_button.click()\n",
    "\n",
    "        # Extract HTML content\n",
    "        html_content = driver.page_source\n",
    "        driver.quit()\n",
    "\n",
    "        soup = BeautifulSoup(html_content,'lxml')\n",
    "        result = {\n",
    "            \"website_url\": website_url,\n",
    "            \"website_name\": self.get_website_name(website_url),\n",
    "            \"website_text\": self.get_html_title_tag(soup)+self.get_html_meta_tags(soup)+self.get_html_heading_tags(soup)+\n",
    "                                    self.get_text_content(soup)\n",
    "        }\n",
    "        \n",
    "        #Convert to Series object and return\n",
    "        return pd.Series(result)\n",
    "    \n",
    "    def get_website_name(self,website_url):\n",
    "        '''\n",
    "        Example: returns \"google\" from \"www.google.com\"\n",
    "        '''\n",
    "        return \"\".join(urlparse(website_url).netloc.split(\".\")[-2])\n",
    "    \n",
    "    def get_html_title_tag(self,soup):\n",
    "        '''Return the text content of <title> tag from a webpage'''\n",
    "        return '. '.join(soup.title.contents)\n",
    "    \n",
    "    def get_html_meta_tags(self,soup):\n",
    "        '''Returns the text content of <meta> tags related to keywords and description from a webpage'''\n",
    "        tags = soup.find_all(lambda tag: (tag.name==\"meta\") & (tag.has_attr('name') & (tag.has_attr('content'))))\n",
    "        content = [str(tag[\"content\"]) for tag in tags if tag[\"name\"] in ['keywords','description']]\n",
    "        return ' '.join(content)\n",
    "    \n",
    "    def get_html_heading_tags(self,soup):\n",
    "        '''returns the text content of heading tags. The assumption is that headings might contain relatively important text.'''\n",
    "        tags = soup.find_all([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "        content = [\" \".join(tag.stripped_strings) for tag in tags]\n",
    "        return ' '.join(content)\n",
    "    \n",
    "    def get_text_content(self,soup):\n",
    "        '''returns the text content of the whole page with some exception to tags. See tags_to_ignore.'''\n",
    "        tags_to_ignore = ['style', 'script', 'head', 'title', 'meta', '[document]',\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\",\"noscript\"]\n",
    "        tags = soup.find_all(text=True)\n",
    "        result = []\n",
    "        for tag in tags:\n",
    "            stripped_tag = tag.strip()\n",
    "            if tag.parent.name not in tags_to_ignore\\\n",
    "                and isinstance(tag, bs4.element.Comment)==False\\\n",
    "                and not stripped_tag.isnumeric()\\\n",
    "                and len(stripped_tag)>0:\n",
    "                result.append(stripped_tag)\n",
    "        return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = text.replace('\\t',' ')\n",
    "    text = text.replace('\\r',' ')\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_generation(website):\n",
    "    try:\n",
    "        scrapTool = ScrapTool()\n",
    "        web = scrapTool.visit_url(website)\n",
    "        text = cleaning_text(web['website_text'])\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print('Error: ',website)\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = [\n",
    "    \n",
    "]\n",
    "\n",
    "for website in websites:\n",
    "    try:\n",
    "        content = content_generation(website)\n",
    "        # content_list = [website,content,\"Financial Crime\"]\n",
    "        # df.loc[len(df)] = content_list\n",
    "    except Exception as e:\n",
    "        print(website,\": \",e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computers and Technology           302\n",
       "Social Networking and Messaging    114\n",
       "Business/Corporate                 106\n",
       "E-Commerce                         101\n",
       "News                                93\n",
       "Law and Government                  83\n",
       "Narcotics                           74\n",
       "Adult                               43\n",
       "Financial Crime                     31\n",
       "Forums                              16\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "category_name = \"Forums\"\n",
    "category_data = df[df[\"Category\"]==category_name]['cleaned_website_text']\n",
    "category_text = ' '.join(category_data)\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(category_text)\n",
    "\n",
    "important_words = wordcloud.words_.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['hack','hacking','hacker','privacy policy','security']\n",
    "words = words + list(important_words)\n",
    "df.loc[df['Category'] == 'Financial Crime', 'cleaned_website_text'] = df.loc[df['Category'] == 'Financial Crime', 'cleaned_website_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "803    black bones cardingblack bones carding dumps c...\n",
       "933    carding fraud wikipediacontents carding fraud ...\n",
       "934    sell cc cvv clubcrd кардинг форум carding foru...\n",
       "935    кардинг форум carding forumкардинг форум cardi...\n",
       "936    кардинг форум carding forumкардинг форум cardi...\n",
       "                             ...                        \n",
       "958    cc vendor qualityour prices usd cards eur card...\n",
       "959    the carding asylumcarding vendors carding manu...\n",
       "960    cardable sites the carding asylumcardable site...\n",
       "961    bank carding the carding asylumbank carding go...\n",
       "962    carding forumcardingforumcarding carding carde...\n",
       "Name: cleaned_website_text, Length: 31, dtype: object"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Category'] == 'Financial Crime'][\"cleaned_website_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"website_classification.csv\",index=False)\n",
    "# computers.to_csv(\"../Hierarchal model/Computer/computer_subcategory_classification.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
