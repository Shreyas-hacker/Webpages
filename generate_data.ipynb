{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4 as bs4\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website_url</th>\n",
       "      <th>cleaned_website_text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.booking.com/index.html?aid=1743217</td>\n",
       "      <td>official site good hotel accommodation big sav...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://travelsites.com/expedia/</td>\n",
       "      <td>expedia hotel book sites like use vacation wor...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://travelsites.com/tripadvisor/</td>\n",
       "      <td>tripadvisor hotel book sites like previously d...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.momondo.in/?ispredir=true</td>\n",
       "      <td>cheap flights search compare flights momondo f...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.ebookers.com/?AFFCID=EBOOKERS-UK.n...</td>\n",
       "      <td>bot create free account create free account si...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>http://www.oldwomen.org/</td>\n",
       "      <td>old nude women porn mature granny sex horny ol...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>http://www.webcamslave.com</td>\n",
       "      <td>bdsm cams bdsm chat bondage cams free bdsm vid...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>http://www.buyeuroporn.com/</td>\n",
       "      <td>porno dvd online european porn dvd cheap adult...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>http://www.analdreamhouse.com/30/03/agecheck/i...</td>\n",
       "      <td>anal dream house anal dream house anal dream h...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>http://www.world-sex-news.com/</td>\n",
       "      <td>world sex news daily sex news adult news eroti...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1408 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            website_url  \\\n",
       "0        https://www.booking.com/index.html?aid=1743217   \n",
       "1                      https://travelsites.com/expedia/   \n",
       "2                  https://travelsites.com/tripadvisor/   \n",
       "3                 https://www.momondo.in/?ispredir=true   \n",
       "4     https://www.ebookers.com/?AFFCID=EBOOKERS-UK.n...   \n",
       "...                                                 ...   \n",
       "1403                           http://www.oldwomen.org/   \n",
       "1404                         http://www.webcamslave.com   \n",
       "1405                        http://www.buyeuroporn.com/   \n",
       "1406  http://www.analdreamhouse.com/30/03/agecheck/i...   \n",
       "1407                     http://www.world-sex-news.com/   \n",
       "\n",
       "                                   cleaned_website_text Category  \n",
       "0     official site good hotel accommodation big sav...   Travel  \n",
       "1     expedia hotel book sites like use vacation wor...   Travel  \n",
       "2     tripadvisor hotel book sites like previously d...   Travel  \n",
       "3     cheap flights search compare flights momondo f...   Travel  \n",
       "4     bot create free account create free account si...   Travel  \n",
       "...                                                 ...      ...  \n",
       "1403  old nude women porn mature granny sex horny ol...    Adult  \n",
       "1404  bdsm cams bdsm chat bondage cams free bdsm vid...    Adult  \n",
       "1405  porno dvd online european porn dvd cheap adult...    Adult  \n",
       "1406  anal dream house anal dream house anal dream h...    Adult  \n",
       "1407  world sex news daily sex news adult news eroti...    Adult  \n",
       "\n",
       "[1408 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('website_classification.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapTool:\n",
    "    def visit_url(self,website_url):\n",
    "        '''\n",
    "        Visit URL. Download the Content. Initialize the beautifulsoup object. Call parsing methods. Return Series object.\n",
    "        '''\n",
    "        content = requests.get(website_url,timeout=60).content\n",
    "        soup = BeautifulSoup(content,'lxml')\n",
    "        result = {\n",
    "            \"website_url\": website_url,\n",
    "            \"website_name\": self.get_website_name(website_url),\n",
    "            \"website_text\": self.get_html_title_tag(soup)+self.get_html_meta_tags(soup)+self.get_html_heading_tags(soup)+\n",
    "                                                               self.get_text_content(soup)\n",
    "        }\n",
    "        \n",
    "        #Convert to Series object and return\n",
    "        return pd.Series(result)\n",
    "    \n",
    "    def get_website_name(self,website_url):\n",
    "        '''\n",
    "        Example: returns \"google\" from \"www.google.com\"\n",
    "        '''\n",
    "        return \"\".join(urlparse(website_url).netloc.split(\".\")[-2])\n",
    "    \n",
    "    def get_html_title_tag(self,soup):\n",
    "        '''Return the text content of <title> tag from a webpage'''\n",
    "        return '. '.join(soup.title.contents)\n",
    "    \n",
    "    def get_html_meta_tags(self,soup):\n",
    "        '''Returns the text content of <meta> tags related to keywords and description from a webpage'''\n",
    "        tags = soup.find_all(lambda tag: (tag.name==\"meta\") & (tag.has_attr('name') & (tag.has_attr('content'))))\n",
    "        content = [str(tag[\"content\"]) for tag in tags if tag[\"name\"] in ['keywords','description']]\n",
    "        return ' '.join(content)\n",
    "    \n",
    "    def get_html_heading_tags(self,soup):\n",
    "        '''returns the text content of heading tags. The assumption is that headings might contain relatively important text.'''\n",
    "        tags = soup.find_all([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "        content = [\" \".join(tag.stripped_strings) for tag in tags]\n",
    "        return ' '.join(content)\n",
    "    \n",
    "    def get_text_content(self,soup):\n",
    "        '''returns the text content of the whole page with some exception to tags. See tags_to_ignore.'''\n",
    "        tags_to_ignore = ['style', 'script', 'head', 'title', 'meta', '[document]',\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\",\"noscript\"]\n",
    "        tags = soup.find_all(text=True)\n",
    "        result = []\n",
    "        for tag in tags:\n",
    "            stripped_tag = tag.strip()\n",
    "            if tag.parent.name not in tags_to_ignore\\\n",
    "                and isinstance(tag, bs4.element.Comment)==False\\\n",
    "                and not stripped_tag.isnumeric()\\\n",
    "                and len(stripped_tag)>0:\n",
    "                result.append(stripped_tag)\n",
    "        return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = text.replace('\\t',' ')\n",
    "    text = text.replace('\\r',' ')\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r' +',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_generation(website):\n",
    "    scrapTool = ScrapTool()\n",
    "    try:\n",
    "        web = dict(scrapTool.visit_url(website))\n",
    "        text = cleaning_text(web['website_text'])\n",
    "        return text\n",
    "    except:\n",
    "        print('Error: ',website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites= ['https://www.reddit.com/r/porn/','https://www.eurogirlsescort.com/#','https://www.adultlook.com/',\n",
    "           'https://adultwork.com/','https://onebackpage.com/']\n",
    "\n",
    "for website in websites:\n",
    "    text = content_generation(website)\n",
    "    web = [website,text,'Adult']\n",
    "    df.loc[len(df)] = web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website_url</th>\n",
       "      <th>cleaned_website_text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>https://www.reddit.com/r/porn/</td>\n",
       "      <td>reddit dive into anythingpornpress j to jump t...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>https://www.eurogirlsescort.com/#</td>\n",
       "      <td>escort directory vip escort girls escort liste...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>https://www.adultlook.com/</td>\n",
       "      <td>escorts escort reviews adultlooktype few keywo...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>https://adultwork.com/</td>\n",
       "      <td>adultworkcom adult service providers erotic co...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>https://onebackpage.com/</td>\n",
       "      <td>onebackpage cars apartments classifieds adult ...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            website_url  \\\n",
       "1441     https://www.reddit.com/r/porn/   \n",
       "1442  https://www.eurogirlsescort.com/#   \n",
       "1443         https://www.adultlook.com/   \n",
       "1444             https://adultwork.com/   \n",
       "1445           https://onebackpage.com/   \n",
       "\n",
       "                                   cleaned_website_text Category  \n",
       "1441  reddit dive into anythingpornpress j to jump t...    Adult  \n",
       "1442  escort directory vip escort girls escort liste...    Adult  \n",
       "1443  escorts escort reviews adultlooktype few keywo...    Adult  \n",
       "1444  adultworkcom adult service providers erotic co...    Adult  \n",
       "1445  onebackpage cars apartments classifieds adult ...    Adult  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('website_classification.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
